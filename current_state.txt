.
├── backend
│   ├── Dockerfile
│   ├── .env
│   ├── __init__.py
│   ├── main.py
│   ├── pyproject.toml
│   └── src
│       ├── client.py
│       ├── functions
│       ├── __init__.py
│       ├── prompts.py
|       ├── utils
│       ├── services.py
│       └── workflows
├── docker-compose.yml
├── frontend
├── llm-output/input

#./docker-compose.yml
services:
  restack-engine:
    image: ghcr.io/restackio/restack:main
    container_name: restack
    restart: always
    networks:
      - restack-network
    ports:
      - "5233:5233"
      - "6233:6233"
      - "7233:7233"

  docker-dind:
    image: docker:24-dind
    privileged: true
    command: ["dockerd", "--host=tcp://0.0.0.0:2375", "--tls=false"]
    networks:
      - restack-network
    volumes:
      - ./llm-output:/app/output:rw

  backend:
    build: ./backend
    environment:
      - OPENAI_KEY=${OPENAI_KEY}
      - DOCKER_HOST=tcp://docker-dind:2375
      - RESTACK_ENGINE_ADDRESS=http://restack:6233
      - RESTACK_TEMPORAL_ADDRESS=http://restack:7233
      - RESTACK_ENGINE_ID = "local"
      - RESTACK_ENGINE_API_KEY = None
      - LLM_OUTPUT_DIR=/app/output
    depends_on:
      - restack-engine
      - docker-dind
    command: ["poetry", "run", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
    networks:
      - restack-network
    volumes:
      - ./llm-output:/app/output:rw
    ports:
      - "8000:8000"

  worker:
    build: ./backend
    environment:
      - OPENAI_KEY=${OPENAI_KEY}
      - DOCKER_HOST=tcp://docker-dind:2375
      - RESTACK_ENGINE_ADDRESS=http://restack:6233
      - RESTACK_TEMPORAL_ADDRESS=http://restack:7233
      - RESTACK_ENGINE_ID = "local"
      - RESTACK_ENGINE_API_KEY = None
      - LLM_OUTPUT_DIR=/app/output
    depends_on:
      - restack-engine
      - docker-dind
      - backend
    command: ["sh", "-c", "sleep 5 && poetry run python -m src.services"]
    networks:
      - restack-network
    volumes:
      - ./llm-output:/app/output:rw

  frontend:
    build: ./frontend
    depends_on:
      - backend
    command: ["npm", "run", "dev"]
    networks:
      - restack-network
    ports:
      - "8080:8080"

networks:
  restack-network:
    driver: bridge

#./backend/Dockerfile
FROM python:3.10-slim

# Set working directory
WORKDIR /app

# Copy Poetry configuration files
COPY pyproject.toml poetry.lock* /app/

# Install dependencies, Docker CLI tools, plus git and tree
RUN apt-get update && apt-get install -y \
    ca-certificates \
    curl \
    gnupg \
    git \
    tree \
    && curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg \
    && echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian bookworm stable" > /etc/apt/sources.list.d/docker.list \
    && apt-get update \
    && apt-get install -y docker-ce-cli docker-buildx-plugin docker-compose-plugin \
    && rm -rf /var/lib/apt/lists/*

# Install Poetry
RUN pip install poetry && poetry install --no-root

# Create and ensure permissions for output directory
RUN mkdir -p /app/output && chmod 777 /app/output

# Copy application files
COPY . /app

# Expose application port
EXPOSE 8000


#./backend/pyproject.toml
# Project metadata
[tool.poetry]
name = "azlon"
version = "0.0.1"
description = "autonomous coding project solver"
authors = [
    "Harrison E. Muchnic <hem9984@nyu.edu>",
]
readme = "readme.md"
packages = [{include = "src"}]

[tool.poetry.dependencies]
python = ">=3.10,<4.0"
restack-ai = "0.0.50"
openai = "1.57.1"
pydantic = "^2.10.3"
fastapi = "0.115.4"  
uvicorn = "^0.22.0"

[tool.poetry.dev-dependencies]
pytest = "6.2"  # Optional: Add if you want to include tests in your example

# Build system configuration
[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

# CLI command configuration
[tool.poetry.scripts]
services = "src.services:run_services"
schedule = "schedule_workflow:run_schedule_workflow"

# ./backend/main.py
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import time
import os

from src.prompts import get_prompts, set_prompts
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions

RESTACK_ENGINE_ADDRESS = os.getenv('RESTACK_ENGINE_ADDRESS')
RESTACK_TEMPORAL_ADDRESS = os.getenv('RESTACK_TEMPORAL_ADDRESS')
RESTACK_ENGINE_ID = os.getenv('RESTACK_ENGINE_ID')
RESTACK_ENGINE_API_KEY = os.getenv('RESTACK_ENGINE_API_KEY')

app = FastAPI()

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000","http://localhost:8080"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

class UserInput(BaseModel):
    user_prompt: str
    test_conditions: str

class PromptsInput(BaseModel):
    generate_code_prompt: str
    validate_output_prompt: str

@app.get("/prompts")
def fetch_prompts():
    return get_prompts()

@app.post("/prompts")
def update_prompts(prompts: PromptsInput):
    set_prompts(prompts.generate_code_prompt, prompts.validate_output_prompt)
    return {"status": "updated"}

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    return JSONResponse(
        status_code=500,
        content={"error": "Internal Server Error."},
        headers={"Access-Control-Allow-Origin": "http://localhost:8080"},
    )

@app.post("/run_workflow")
async def run_workflow(params: UserInput):
    connection_options = CloudConnectionOptions(
    engine_id=RESTACK_ENGINE_ID,
    api_key=RESTACK_ENGINE_API_KEY,
    address=RESTACK_TEMPORAL_ADDRESS,
    api_address=RESTACK_ENGINE_ADDRESS,
    temporal_namespace="default")

    # Initialize Restack with these options options=connection_options
    client = Restack(connection_options)
    try:
        workflow_id = f"{int(time.time() * 1000)}-AutonomousCodingWorkflow"
        runId = await client.schedule_workflow(
            workflow_name="AutonomousCodingWorkflow",
            workflow_id=workflow_id,
            input=params.dict()
        )
        result = await client.get_workflow_result(workflow_id=workflow_id, run_id=runId)
        return {"workflow_id": workflow_id, "result": result}
    except Exception as e:
        # If engine connection or workflow run fails, a 500 error is raised
        # The global_exception_handler ensures CORS headers are included.
        raise HTTPException(status_code=500, detail="Failed to connect to Restack engine or run workflow.")

#./backend/src/services.py
import traceback
import asyncio
import time
from src.client import client
from src.functions.functions import (
    generate_code, run_locally, validate_output, pre_flight_run
)
from src.workflows.workflow import AutonomousCodingWorkflow

async def main():
    try:
        await client.start_service(
            workflows=[AutonomousCodingWorkflow],
            functions=[generate_code, run_locally, validate_output, pre_flight_run],
        )
    except Exception as e:
        print(f"Error starting service: traceback: {traceback.format_exc()}")
        print(f"Error starting service: {e}")
        raise

def run_services():
    try:
        asyncio.run(main())
    except Exception as e:
        print(f"Service failed: {e}")
    # Keep the process alive for inspection
    while True:
        time.sleep(1)

if __name__ == "__main__":
    run_services()


#./backend/src/client.py
import os 
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions

RESTACK_TEMPORAL_ADDRESS = os.getenv('RESTACK_TEMPORAL_ADDRESS')
RESTACK_ENGINE_ADDRESS = os.getenv('RESTACK_ENGINE_ADDRESS')
RESTACK_ENGINE_ID = os.getenv('RESTACK_ENGINE_ID')
RESTACK_ENGINE_API_KEY = os.getenv('RESTACK_ENGINE_API_KEY')

# src/client.py
connection_options = CloudConnectionOptions(
    engine_id=RESTACK_ENGINE_ID,
    api_key=RESTACK_ENGINE_API_KEY,
    address=RESTACK_TEMPORAL_ADDRESS,
    api_address=RESTACK_ENGINE_ADDRESS,
    temporal_namespace="default")

# Initialize Restack with these options options=connection_options
client = Restack(connection_options)

# ./backend/src/prompts.py
# Store defaults here
default_generate_code_prompt = """You are an autonomous coding agent.

The user prompt: {user_prompt}
The test conditions: {test_conditions}

You must produce a Docker environment and code that meets the user's test conditions.

**Additional Requirements**:
- Start by creating a `readme.md` file as your first file in the files array. This `readme.md` should begin with `#./readme.md` and contain:
  - A brief summary of the user's prompt.
  - A brief step-by-step plan of what you intend to do to meet the test conditions.
- Use a stable base Docker image: `FROM python:3.10-slim`.
- Install any necessary dependencies in the Dockerfile.
- Generate any configuration files (like `pyproject.toml` or `requirements.txt`) before the main Python files, if needed.
- Each file must start with `#./<filename>` on the first line. For example:
  `#./main.py`
  `print('hello world')`
- The Dockerfile should define an ENTRYPOINT that runs the main script or commands automatically so that running the container (e.g. `docker run ...`) immediately produces the final output required by the test conditions.
- Ensure the output visible on stdout fulfills the test conditions without further intervention.

**Return JSON strictly matching this schema**:
{{
  "dockerfile": "<string>",
  "files": [
    {{
      "filename": "<string>",
      "content": "<string>"
    }},
    ...
  ]
}}

**Order of files**:
1. `readme.md` (with reasoning and plan)
2. Any configuration files (like `pyproject.toml` or `requirements.txt`)
3. Your main Python application files

**Example**:
{{
  "dockerfile": "FROM python:3.10-slim\\n... ENTRYPOINT [\\"python3\\", \\"main.py\\"]",
  "files": [
    {{
      "filename": "readme.md",
      "content": "#./readme.md\\nThis is my reasoning..."
    }},
    {{
      "filename": "pyproject.toml",
      "content": "#./pyproject.toml\\n..."
    }},
    {{
      "filename": "main.py",
      "content": "#./main.py\\nprint('hello world')"
    }}
  ]
}}
"""

default_validate_output_prompt = """The test conditions: {test_conditions}

dockerfile:
{dockerfile}

files:
{files_str}

output:
{output}

If all test conditions are met, return exactly:
{{ "result": true, "dockerfile": null, "files": null }}

Otherwise (if you need to fix or add files, modify the dockerfile, etc.), return exactly:
{{
  "result": false,
  "dockerfile": "FROM python:3.10-slim\\n...",
  "files": [
    {{
      "filename": "filename.ext",
      "content": "#./filename.ext\\n..."
    }}
  ]
}}

You may add, remove, or modify multiple files as needed when returning false. Just ensure you follow the same schema and format strictly. Do not add extra commentary or keys.
If returning null for dockerfile or files, use JSON null, not a string."""

# Storing the current prompts in memory for simplicity.
current_generate_code_prompt = default_generate_code_prompt
current_validate_output_prompt = default_validate_output_prompt

def get_prompts():
    return {
        "generate_code_prompt": current_generate_code_prompt,
        "validate_output_prompt": current_validate_output_prompt
    }

def set_prompts(generate_code_prompt: str, validate_output_prompt: str):
    global current_generate_code_prompt, current_validate_output_prompt
    current_generate_code_prompt = generate_code_prompt
    current_validate_output_prompt = validate_output_prompt

#./backend/src/workflows/workflow.py
from restack_ai.workflow import workflow, import_functions, log
from dataclasses import dataclass
from datetime import timedelta
import os

with import_functions():
    from src.functions.functions import (
        generate_code, run_locally, validate_output, pre_flight_run,
        GenerateCodeInput, RunCodeInput, ValidateOutputInput, PreFlightOutput
    )

@dataclass
class WorkflowInputParams:
    user_prompt: str
    test_conditions: str

@workflow.defn()
class AutonomousCodingWorkflow:
    @workflow.run
    async def run(self, input: WorkflowInputParams):
        log.info("AutonomousCodingWorkflow started", input=input)

        # 0) OPTIONAL PRE-FLIGHT STEP
        input_dir = os.path.join(os.environ.get("LLM_OUTPUT_DIR", "/app/output"), "input")
        has_user_files = False
        if os.path.isdir(input_dir):
            for root, dirs, files in os.walk(input_dir):
                if files:
                    has_user_files = True
                    break

        pre_flight_result = None
        if has_user_files:
            log.info("Pre-flight: found user-provided code. Let's build/run it to see what happens.")
            pre_flight_result: PreFlightOutput = await workflow.step(
                pre_flight_run,
                start_to_close_timeout=timedelta(seconds=300)
            )
            log.info("Pre-flight completed. Output:\n" + pre_flight_result.run_output)

        # 1) Generate code (first iteration)
        pre_flight_output_text = ""
        if pre_flight_result:
            pre_flight_output_text = (
                f"\n\nWe ran the user-provided code. The container output was:\n\n"
                f"{pre_flight_result.run_output}\n\n"
                f"The code structure is:\n{pre_flight_result.dir_tree}\n\n"
                "Please modify or create a Dockerfile if needed, and adjust the code to meet new test conditions.\n"
            )

        gen_output = await workflow.step(
            generate_code,
            GenerateCodeInput(
                user_prompt=input.user_prompt + pre_flight_output_text,
                test_conditions=input.test_conditions
            ),
            start_to_close_timeout=timedelta(seconds=300)
        )

        dockerfile = gen_output.dockerfile
        files = gen_output.files  # list of {"filename":..., "content":...}

        iteration_count = 0
        max_iterations = 20

        while iteration_count < max_iterations:
            iteration_count += 1
            log.info(f"Iteration {iteration_count} start")

            run_output = await workflow.step(
                run_locally,
                RunCodeInput(dockerfile=dockerfile, files=files),
                start_to_close_timeout=timedelta(seconds=300)
            )

            val_output = await workflow.step(
                validate_output,
                ValidateOutputInput(
                    dockerfile=dockerfile,
                    files=files,
                    output=run_output.output,
                    test_conditions=input.test_conditions,
                    iteration=iteration_count
                ),
                start_to_close_timeout=timedelta(seconds=300)
            )

            if val_output.result:
                log.info("AutonomousCodingWorkflow completed successfully")
                return True
            else:
                changed_files = val_output.files if val_output.files else []
                if val_output.dockerfile:
                    dockerfile = val_output.dockerfile

                # Merge changes
                for changed_file in changed_files:
                    changed_filename = changed_file["filename"]
                    changed_content = changed_file["content"]
                    found = False
                    for i, existing_file in enumerate(files):
                        if existing_file["filename"] == changed_filename:
                            files[i]["content"] = changed_content
                            found = True
                            break
                    if not found:
                        files.append({"filename": changed_filename, "content": changed_content})

        log.warn("AutonomousCodingWorkflow reached max iterations without success")
        return False


#./backend/src/functions/functions.py
from restack_ai.function import function, log
from dataclasses import dataclass
import os
import openai
import json
import subprocess
from datetime import datetime
from typing import List, Optional

from pydantic import BaseModel

from src.prompts import current_generate_code_prompt, current_validate_output_prompt
from src.utils.file_handling import (
    run_tree_command,
    initialize_git_repo,
    prepare_codebase_merge,
    collect_input_files,
    build_files_str
)

openai.api_key = os.environ.get("OPENAI_KEY")
from openai import OpenAI
client = OpenAI(api_key=openai.api_key)

class FileItem(BaseModel):
    filename: str
    content: str

    class Config:
        extra = "forbid"
        schema_extra = {
            "type": "object",
            "properties": {
                "filename": {"type": "string"},
                "content": {"type": "string"}
            },
            "required": ["filename", "content"],
            "additionalProperties": False
        }

class GenerateCodeSchema(BaseModel):
    dockerfile: str
    files: List[FileItem]
    
    class Config:
        extra = "forbid"
        schema_extra = {
            "type": "object",
            "properties": {
                "dockerfile": {"type": "string"},
                "files": {
                    "type": "array",
                    "items": {"$ref": "#/$defs/FileItem"}
                }
            },
            "required": ["dockerfile", "files"],
            "additionalProperties": False,
            "$defs": {
                "FileItem": {
                    "type": "object",
                    "properties": {
                        "filename": {"type": "string"},
                        "content": {"type": "string"}
                    },
                    "required": ["filename", "content"],
                    "additionalProperties": False
                }
            }
        }

class ValidateOutputSchema(BaseModel):
    result: bool
    dockerfile: Optional[str] = None
    files: Optional[List[FileItem]] = None
    
    class Config:
        extra = "forbid"
        schema_extra = {
            "type": "object",
            "properties": {
                "result": {"type": "boolean"},
                "dockerfile": {
                    "anyOf": [
                        {"type": "string"},
                        {"type": "null"}
                    ]
                },
                "files": {
                    "anyOf": [
                        {
                            "type": "array",
                            "items": {"$ref": "#/$defs/FileItem"}
                        },
                        {"type": "null"}
                    ]
                }
            },
            "required": ["result", "dockerfile", "files"],
            "additionalProperties": False,
            "$defs": {
                "FileItem": {
                    "type": "object",
                    "properties": {
                        "filename": {"type": "string"},
                        "content": {"type": "string"}
                    },
                    "required": ["filename", "content"],
                    "additionalProperties": False
                }
            }
        }

@dataclass
class GenerateCodeInput:
    user_prompt: str
    test_conditions: str

@dataclass
class GenerateCodeOutput:
    dockerfile: str
    files: list

@dataclass
class RunCodeInput:
    dockerfile: str
    files: list

@dataclass
class RunCodeOutput:
    output: str

@dataclass
class ValidateOutputInput:
    dockerfile: str
    files: list
    output: str
    test_conditions: str
    iteration: int

@dataclass
class ValidateOutputOutput:
    result: bool
    dockerfile: Optional[str] = None
    files: Optional[list] = None

#
# generate_code
#
@function.defn()
async def generate_code(input: GenerateCodeInput) -> GenerateCodeOutput:
    log.info("generate_code started", input=input)

    prompt = current_generate_code_prompt.format(
        user_prompt=input.user_prompt,
        test_conditions=input.test_conditions
    )

    completion = client.beta.chat.completions.parse(
        model="gpt-4o-2024-08-06",
        messages=[
            {
                "role": "system",
                "content": "You are the initial of an autonomous coding assistant agent. Generate complete code that will run."
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        response_format=GenerateCodeSchema
    )

    result = completion.choices[0].message
    if result.refusal:
        raise RuntimeError("Model refused to generate code.")
    data = result.parsed

    files_list = [{"filename": f.filename, "content": f.content} for f in data.files]

    return GenerateCodeOutput(dockerfile=data.dockerfile, files=files_list)

#
# run_locally
#
@function.defn()
async def run_locally(input: RunCodeInput) -> RunCodeOutput:
    log.info("run_locally started", input=input)
    
    base_output_dir = os.environ.get("LLM_OUTPUT_DIR", "/app/output")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_folder = os.path.join(base_output_dir, f"llm_run_{timestamp}")
    os.makedirs(run_folder, exist_ok=True)
    
    # Write Dockerfile
    dockerfile_path = os.path.join(run_folder, "Dockerfile")
    os.makedirs(os.path.dirname(dockerfile_path), exist_ok=True)
    with open(dockerfile_path, "w", encoding="utf-8") as f:
        f.write(input.dockerfile)
    log.info(f"Wrote Dockerfile to {dockerfile_path}")

    # Initialize Git
    initialize_git_repo(run_folder)
    # We'll allow empty commit in case there's nothing else to add
    commit_cmd = ["git", "add", "-A"]
    subprocess.run(commit_cmd, cwd=run_folder, check=True)
    commit_cmd = ["git", "commit", "-m", "Initial Dockerfile commit", "--allow-empty"]
    subprocess.run(commit_cmd, cwd=run_folder, check=True)

    # Merge in the LLM's new files
    prepare_codebase_merge(
        repo_path=run_folder,
        llm_files=input.files
    )

    # Show tree for debugging
    tree_output = run_tree_command(run_folder)
    log.info(f"Directory after merge:\n{tree_output}")

    # Docker build
    build_cmd = ["docker", "build", "-t", "myapp", run_folder]
    build_process = subprocess.run(build_cmd, capture_output=True, text=True)
    if build_process.returncode != 0:
        return RunCodeOutput(output=build_process.stderr or build_process.stdout)
    
    # Docker run
    run_cmd = ["docker", "run", "--rm", "myapp"]
    run_process = subprocess.run(run_cmd, capture_output=True, text=True)
    if run_process.returncode != 0:
        return RunCodeOutput(output=run_process.stderr or run_process.stdout)
    
    return RunCodeOutput(output=run_process.stdout)

#
# validate_output
#
@function.defn()
async def validate_output(input: ValidateOutputInput) -> ValidateOutputOutput:
    log.info("validate_output started", input=input)

    files_str = build_files_str(
        dockerfile=input.dockerfile,
        files=input.files,
        iteration=input.iteration
    )

    validation_prompt = current_validate_output_prompt.format(
        test_conditions=input.test_conditions,
        dockerfile=input.dockerfile,
        files_str=files_str,
        output=input.output
    )

    completion = client.beta.chat.completions.parse(
        model="gpt-4o-2024-08-06",
        messages=[
            {
                "role": "system",
                "content": "You are an iteration of an autonomous coding assistant agent. If you change any files, provide complete file content replacements. Append a brief explanation at the bottom of readme.md about what you tried."
            },
            {
                "role": "user",
                "content": validation_prompt
            }
        ],
        response_format=ValidateOutputSchema
    )

    result = completion.choices[0].message
    if result.refusal:
        return ValidateOutputOutput(result=False)

    data = result.parsed
    updated_files = [{"filename": f.filename, "content": f.content} for f in data.files] if data.files is not None else None

    return ValidateOutputOutput(
        result=data.result,
        dockerfile=data.dockerfile,
        files=updated_files
    )

#
# pre_flight_run
#
@dataclass
class PreFlightOutput:
    dir_tree: str
    run_output: str

@function.defn()
async def pre_flight_run() -> PreFlightOutput:
    log.info("pre_flight_run started")

    base_output_dir = os.environ.get("LLM_OUTPUT_DIR", "/app/output")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_folder = os.path.join(base_output_dir, f"preflight_{timestamp}")
    os.makedirs(run_folder, exist_ok=True)

    user_json = collect_input_files()
    if not user_json["dockerfile"].strip():
        default_dockerfile = """FROM python:3.10-slim
WORKDIR /app
COPY . .
RUN apt-get update && apt-get install -y python3 python3-pip
ENTRYPOINT ["python3","main.py"]
"""
        user_json["dockerfile"] = default_dockerfile

    dockerfile_path = os.path.join(run_folder, "Dockerfile")
    with open(dockerfile_path, "w", encoding="utf-8") as df:
        df.write(user_json["dockerfile"])

    initialize_git_repo(run_folder)
    subprocess.run(["git", "add", "-A"], cwd=run_folder, check=True)
    # allow empty commit
    subprocess.run(["git", "commit", "-m", "pre-flight base Dockerfile", "--allow-empty"], cwd=run_folder, check=True)

    # Merge user files
    prepare_codebase_merge(
        repo_path=run_folder,
        llm_files=user_json["files"]
    )

    build_cmd = ["docker", "build", "-t", "preflight_app", run_folder]
    build_process = subprocess.run(build_cmd, capture_output=True, text=True)
    if build_process.returncode != 0:
        dir_tree = run_tree_command(run_folder)
        return PreFlightOutput(
            dir_tree=dir_tree,
            run_output=(build_process.stderr or build_process.stdout)
        )

    run_cmd = ["docker", "run", "--rm", "preflight_app"]
    run_process = subprocess.run(run_cmd, capture_output=True, text=True)
    dir_tree = run_tree_command(run_folder)
    if run_process.returncode != 0:
        return PreFlightOutput(dir_tree=dir_tree, run_output=(run_process.stderr or run_process.stdout))

    return PreFlightOutput(dir_tree=dir_tree, run_output=run_process.stdout)

#./backend/src/utils/file_handling.py
import os
import re
import json
import subprocess
from typing import List, Dict
import logging

logger = logging.getLogger(__name__)

# ---------------------------------------
# Git & tree utilities
# ---------------------------------------

def run_tree_command(directory: str) -> str:
    """
    Runs the `tree` command against the given directory
    and returns its stdout as a string.
    """
    try:
        result = subprocess.run(
            ["tree", directory],
            capture_output=True,
            text=True,
            check=True
        )
        return result.stdout
    except Exception as e:
        logger.warning(f"Failed to run 'tree' in {directory}: {e}")
        return ""

def initialize_git_repo(repo_path: str) -> None:
    """
    Initializes a new Git repository in repo_path,
    using 'main' as the default branch to avoid warnings.
    Configures user name/email to allow committing.
    """
    # The key argument is --initial-branch=main
    subprocess.run(["git", "init", "--initial-branch=main"], cwd=repo_path, check=True)
    subprocess.run(["git", "config", "user.name", "AzlonBot"], cwd=repo_path, check=True)
    subprocess.run(["git", "config", "user.email", "azlon@local"], cwd=repo_path, check=True)

def commit_all_changes(repo_path: str, message: str, allow_empty: bool = True) -> None:
    """
    Adds all files in repo_path to staging and commits with the given message.
    If allow_empty=True, we pass --allow-empty so it won't fail if no changes exist.
    """
    subprocess.run(["git", "add", "-A"], cwd=repo_path, check=True)
    commit_cmd = ["git", "commit", "-m", message]
    if allow_empty:
        commit_cmd.append("--allow-empty")
    subprocess.run(commit_cmd, cwd=repo_path, check=True)

def auto_merge_with_llm_changes(repo_path: str) -> None:
    """
    Merges the 'llm-changes' branch into the 'main' branch
    with automatic conflict resolution favoring the LLM changes.
    """
    subprocess.run(
        ["git", "merge", "llm-changes", "-X", "theirs", "--allow-unrelated-histories"],
        cwd=repo_path,
        check=True
    )

def prepare_codebase_merge(repo_path: str, llm_files: List[Dict[str, str]]) -> None:
    """
    1) Commits existing (base) code on 'main'
    2) Creates & switches to 'llm-changes'
    3) Writes the new LLM files, commits them
    4) Switches back to 'main', merges with 'llm-changes' (favor LLM)
    """
    # We are already on 'main' from initialize_git_repo, but let's forcibly check out:
    subprocess.run(["git", "checkout", "main"], cwd=repo_path, check=True)
    commit_all_changes(repo_path, "Base commit", allow_empty=True)

    # Create or switch to 'llm-changes' branch
    subprocess.run(["git", "checkout", "-b", "llm-changes"], cwd=repo_path, check=True)

    for f in llm_files:
        full_path = os.path.join(repo_path, f["filename"])
        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        with open(full_path, "w", encoding="utf-8") as file_out:
            file_out.write(f["content"])

    commit_all_changes(repo_path, "LLM changes", allow_empty=True)

    subprocess.run(["git", "checkout", "main"], cwd=repo_path, check=True)
    auto_merge_with_llm_changes(repo_path)

# ---------------------------------------
# Collect user input code
# ---------------------------------------

def collect_input_files() -> Dict[str, any]:
    """
    Scans ./llm-output/input and returns a dict:
    {
      "dockerfile": <str or empty if none found>,
      "files": [ { "filename": "path/file", "content": "..." }, ... ]
    }
    We skip any .git directories or their contents.
    """
    base_input_dir = os.path.join(
        os.environ.get("LLM_OUTPUT_DIR", "/app/output"), "input"
    )
    dockerfile_contents = ""
    collected_files = []

    if not os.path.isdir(base_input_dir):
        return {"dockerfile": "", "files": []}

    for root, dirs, files in os.walk(base_input_dir):
        # skip any .git directories
        if ".git" in dirs:
            dirs.remove(".git")

        for fname in files:
            # If it's in a .git folder deeper, skip
            if ".git" in root:
                continue

            full_path = os.path.join(root, fname)
            rel_path = os.path.relpath(full_path, base_input_dir)

            try:
                with open(full_path, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
            except:
                content = ""

            if fname.lower() == "dockerfile":
                if not dockerfile_contents:
                    dockerfile_contents = content
            else:
                collected_files.append({
                    "filename": rel_path,
                    "content": content
                })

    return {
        "dockerfile": dockerfile_contents,
        "files": collected_files
    }

# ---------------------------------------
# Summaries & partial code handling
# ---------------------------------------

def extract_function_signatures(content: str) -> str:
    """
    Return lines that look like function or class definitions.
    E.g. 'def my_func(...):' or 'class MyClass:'
    """
    pattern = re.compile(r'^\s*(def|class)\s+\w+.*:')
    signatures = []
    for line in content.splitlines():
        if pattern.match(line):
            signatures.append(line.strip())
    if not signatures:
        return "# (No function/class definitions found)"
    return "\n".join(signatures)

def build_files_str(dockerfile: str, files: List[Dict[str, str]], iteration: int) -> str:
    """
    For 'validate_output':
      - iteration >= 6 => full content of Dockerfile + files
      - iteration < 6 => partial
        * If file ext in {".csv",".numpy",".npy",".tsv"} => only first 2 lines + '...'
        * If > 50 lines => function/class signatures only
    Returns a JSON array string that we inject into the prompt.
    """
    if iteration >= 6:
        return json.dumps(
            [{"filename": "Dockerfile", "content": dockerfile}] + files,
            indent=2
        )

    special_exts = {".csv", ".numpy", ".npy", ".tsv"}
    truncated_list = []

    for f in files:
        ext = os.path.splitext(f["filename"])[1].lower()
        lines = f["content"].splitlines()

        # If it's a special extension, only first 2 lines
        if ext in special_exts:
            snippet = "\n".join(lines[:2]) + "\n..." if len(lines) >= 2 else "... (no lines found)"
            truncated_list.append({
                "filename": f["filename"],
                "content": snippet
            })
            continue

        # If it's > 50 lines, only function/class signatures
        if len(lines) > 50:
            sigs = extract_function_signatures(f["content"])
            snippet = "# (File truncated for brevity)\n" + sigs
            truncated_list.append({
                "filename": f["filename"],
                "content": snippet
            })
        else:
            truncated_list.append({
                "filename": f["filename"],
                "content": f["content"]
            })

    docker_lines = dockerfile.splitlines()
    if len(docker_lines) > 50:
        df_snippet = "\n".join(docker_lines[:10]) + "\n# (Dockerfile truncated)"
    else:
        df_snippet = dockerfile

    final_list = [{"filename": "Dockerfile", "content": df_snippet}] + truncated_list
    return json.dumps(final_list, indent=2)


HOW IT WORKS:
User inputs user_prompt and test_conditions in the frontend UI (and optionally adds the starting files to ./llm-output/input directory) then clicks "run workflow". The autonomous workflow begins, and iterates until the LLM deems that the test conditions are fulfilled.